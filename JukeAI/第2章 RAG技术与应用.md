# 第2章 RAG技术与应用

## 💡 学习目标

1. 大模型应用开发的三种模式（范式）
2. RAG技术概述
3. RAG核心原理与流程
4. NativeRAG
5. LangChain快速搭建本地知识库
6. 三大阶段有效提升RAG质量的方法



## 1. 大模型应用开发的三种模式


这张图其实在说明 **LLM 应用开发的三种增强手段**，以及它们解决的问题：

### 1. 海量知识预训练 → LLM → AI应用

- 左边：模型先在大规模数据上做预训练，学到通用语言知识。
- 中间：得到一个通用大语言模型（LLM）。
- 右边：这个 LLM 可以支持各种 AI 应用（聊天、搜索、问答、写作等）。

但是，**LLM 输出有时会出错**，原因主要有三类：

1. **问题没问清楚** → Prompt 工程
   - 用户的问题不明确，模型理解不到位。
   - 通过 **改写 Prompt**（提示词设计），让模型更好理解需求。
2. **模型缺乏背景知识** → RAG（检索增强生成）
   - 模型参数里没有相关知识，导致回答错误或不完整。
   - 通过 **RAG** 把外部知识库检索结果喂给模型，弥补缺口。
3. **模型能力本身不足** → Fine-tuning（微调）
   - 模型规模或训练目标不足以支持某类任务。
   - 需要用专门数据集做 **微调**，提升在目标任务上的能力。

### 总结一句话：

- **Prompt**：解决“用户提问不清楚”；
- **RAG**：解决“模型缺乏外部知识”；
- **Fine-tuning**：解决“模型自身能力不足”。

三者是 **大模型应用开发的三种模式**，也是你在 `RAG 技术与应用` 那一章笔记里提到的学习目标的第一条。

Q：模型能力本身不足  具体指的是什么？

A：当图里说「模型能力本身不足」，指的是 **LLM 的固有限制**，即便问题问得清楚、即便补充了外部知识，它还是可能答不好。具体包括以下几类：

## 1. **参数知识有限**

- LLM 预训练时学习到的知识是“冻结”的。
- 对一些 **新知识、专业知识**，模型根本没学过。
- 即便通过 RAG 补充上下文，它在 **推理能力** 或 **领域理解** 上可能还是不足。

## 2. **推理能力不足**

- LLM 的逻辑链条有限，处理复杂数学推理、多步骤规划时容易出错。
- 例子：让 GPT-3.5 做多步骤的数理推导，常常算错。

## 3. **任务适配性不足**

- 预训练模型是“通用型”的，但对特定任务（金融风控、医疗问答、法律写作）没有专门优化。
- 导致输出风格、格式、专业性不达标。

## 4. **安全性与对齐不足**

- 模型可能产生有害回答、幻觉内容、不符合企业标准的输出。
- 这属于 **对齐能力不足**，需要再训练（RLHF/DPO）来弥补。

## 5. **性能瓶颈**

- 模型规模有限时，记忆、长文本处理、跨模态（图+文）等能力会不足。
- 需要通过更大模型，或特定的微调（Fine-tuning）来提升。

## 🧩 总结

所谓 **模型能力本身不足**，主要指：

- **没学到 → 知识不足**
- **不会推 → 推理不足**
- **不适配 → 任务不对口**
- **不稳健 → 对齐/安全不足**
- **不够强 → 规模与性能不足**

这时就不能单靠 Prompt 或 RAG，而必须通过 **Fine-tuning（微调）** 或 **训练更强的模型** 来提升。

## 2. 什么是 RAG ？

RAG（Retrieval-Augmented Generation）

RAG（检索增强生成）就是：模型不知道答案时，先去“翻资料”，再结合自己的能力来回答问题。

📌 可以这么类比：

- **纯 LLM**：就像一个靠记忆考试的学生，只能答出自己学过的。
- **RAG**：就像学生考试时允许带一本资料书，先查书，再用自己的语言写答案。

👉 **RAG = 检索外部知识 + LLM生成答案**，解决的是 **模型记忆和知识不足** 的问题，是让 LLM 真正落地应用的核心模式之一。

